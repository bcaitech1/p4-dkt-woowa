{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e560942c-44aa-40bf-af49-40156bc7603f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "import time\n",
    "from datetime import datetime\n",
    "import random\n",
    "\n",
    "import pickle\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import os, random, torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "df6e97bd-2c5e-4281-b3d8-5d1a0d98d18f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setSeeds(seed = 42):\n",
    "    # 랜덤 시드를 설정하여 매 코드를 실행할 때마다 동일한 결과를 얻게 합니다.\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)    \n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "086f0064-7365-4f44-b668-c2fa8bce54d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "setSeeds(seed = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "daef732b-9f3f-4b22-9888-6de16e56cfcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 0 ns, sys: 0 ns, total: 0 ns\n",
      "Wall time: 12.6 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "dtype = {\n",
    "    'userID': 'int16',\n",
    "    'answerCode': 'int8',\n",
    "    'KnowledgeTag': 'int16'\n",
    "}   \n",
    "\n",
    "# 데이터 경로 \n",
    "train_path = '../input/data/train_dataset/train_data.csv'\n",
    "test_path = '../input/data/train_dataset/test_data.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4eef52d5-9a07-431c-aa4b-9b738980222b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 10.5 s, sys: 696 ms, total: 11.1 s\n",
      "Wall time: 11.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#데이터를 날짜/시간 포맷에 대한 명시적인 설정없이 그냥 pandas의 read_csv() 함수로 읽어와서 DataFrame을 만들 경우 아래와 같이 object 데이터 형태로 불어오게 됩니다\n",
    "\n",
    "df = pd.read_csv(train_path)\n",
    "df = df.sort_values(by=['userID', 'Timestamp']).reset_index(drop=True)\n",
    "test_df = pd.read_csv(test_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "28d86eda-9ead-42a3-8b0c-ea66c1882f85",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_df = pd.concat([df, test_df[test_df.answerCode!=-1]], ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a6b64e9-e6c1-4483-8732-1946bfb0130b",
   "metadata": {},
   "source": [
    "### train, val split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "184f008f-137b-4401-bd12-7b49a22d92d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 임시 split function\n",
    "def get_train_val_row_ids(df, test_df):\n",
    "    train_size = int(len(df.groupby('userID'))*0.8)\n",
    "    val_size = len(df.groupby('userID')) - train_size\n",
    "    print(f'train_size = {train_size}, val_size = {val_size}')\n",
    "    \n",
    "    end_id_by_user = df.groupby('userID').apply(lambda x: x.index.values[-1]).reset_index()\n",
    "    test_end_id_by_user = test_df.groupby('userID').apply(lambda x: x.index.values[-1]).reset_index()\n",
    "\n",
    "    train_last = df.loc[end_id_by_user[0]]\n",
    "    test_last = test_df.loc[test_end_id_by_user[0]]\n",
    "\n",
    "    test_item_set = set(test_last.assessmentItemID.unique())\n",
    "\n",
    "    user_id_lst = []\n",
    "    for end_id in end_id_by_user[0]:\n",
    "        if df.loc[end_id].assessmentItemID in test_item_set:\n",
    "            user_id_lst.append(df.loc[end_id].userID)\n",
    "\n",
    "    random.seed(0)\n",
    "    random.shuffle(user_id_lst)\n",
    "    print(f'test set 과 같은 문제를 예측하는 user 수 = {len(user_id_lst)}')\n",
    "    val_user_ids = random.sample(user_id_lst, val_size)\n",
    "    print(val_user_ids[:5])  \n",
    "\n",
    "    train_user_ids = list(set(df.userID.unique())-set(val_user_ids))\n",
    "    print(train_user_ids[:5])\n",
    "\n",
    "    # 학습 과정에서 학습 샘플을 생성하기 위해서 필요한 유저별 row index를 저장\n",
    "    row_ids_by_user_id = df.groupby('userID').apply(lambda x: x.index.values)\n",
    "    row_ids_by_user_id = row_ids_by_user_id.reset_index()\n",
    "\n",
    "    train_ids = []\n",
    "    for ids in row_ids_by_user_id[row_ids_by_user_id.userID.isin(train_user_ids)][0]:\n",
    "        train_ids.extend(ids)\n",
    "    print(f'train_ids_len = {len(train_ids)}')\n",
    "\n",
    "    val_ids = []\n",
    "    for ids in row_ids_by_user_id[row_ids_by_user_id.userID.isin(val_user_ids)][0]:\n",
    "        val_ids.extend(ids)\n",
    "    print(f'val_ids_len = {len(val_ids)}')\n",
    "    \n",
    "    return train_ids, val_ids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6fe77fb8-cf2a-42cc-b554-88a0851b4c12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mktime: 시간의 부동 소수점을 나타내는 데 사용 시간 (초)을 리턴\n",
    "def convert_time(s):\n",
    "    timestamp = time.mktime(datetime.strptime(s, '%Y-%m-%d %H:%M:%S').timetuple())\n",
    "    return int(timestamp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "926bed42-b0b1-46c0-a023-fa94f4086514",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- 시간 feature\n",
    "df['time_stamp'] = df['Timestamp'].apply(convert_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "031171f2-3ef6-4381-8740-9bf967200e5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_engineering(df, total_df):\n",
    "    # 학습 과정에서 학습 샘플을 생성하기 위해서 필요한 유저별 row index를 저장\n",
    "    row_ids_by_user_id = df.groupby('userID').apply(lambda x: x.index.values)\n",
    "    \n",
    "    # 학습 과정에서 학습 샘플을 생성하기 위해서 필요한 유저별 시작 row index를 저장\n",
    "    start_row_id_by_user_id = df.groupby('userID').apply(lambda x:x.index.values[0])\n",
    "    \n",
    "    df['day'] = df.Timestamp.apply(lambda x:x.split()[0])\n",
    "    \n",
    "    # -- 정확도 feature, 이거는 전체 데이터 필요할 듯\n",
    "    acc_by_test_id = total_df.groupby('testId').answerCode.mean()\n",
    "    acc_by_assessment_item_id = total_df.groupby('assessmentItemID').answerCode.mean()\n",
    "    acc_by_tag = total_df.groupby('KnowledgeTag').answerCode.mean()\n",
    "    df['acc_avg_by_test_id'] = df['testId'].map(acc_by_test_id)\n",
    "    df['acc_avg_by_assessment_item_id'] = df['assessmentItemID'].map(acc_by_assessment_item_id)\n",
    "    df['acc_avg_by_tag'] = df['KnowledgeTag'].map(acc_by_tag)\n",
    "    \n",
    "    \n",
    "    # -- 상대적 feature\n",
    "    df['relative_answered_correctly'] = df['answerCode'] - df['acc_avg_by_assessment_item_id']\n",
    "    \n",
    "    \n",
    "    # -- 과거 feature\n",
    "    # user 별 이전 문제까지 정답 횟수\n",
    "    df['prior_acc_count'] = df.groupby('userID')['answerCode'].cumsum().shift(fill_value=0)\n",
    "\n",
    "    # 이전까지 푼 문제 수\n",
    "    df['content'] = [1]*len(df)\n",
    "    df['prior_quest_count'] = df.groupby('userID')['content'].cumsum().shift(fill_value=0)\n",
    "\n",
    "    # 이전 문제까지의 정답률\n",
    "    df['prior_acc'] = (df['prior_acc_count'] / df['prior_quest_count']).fillna(0)\n",
    "    \n",
    "    # 이전 문제 상대적인(relative) 정답률\n",
    "    df['prior_relative_acc_sum'] = df.groupby('userID')['relative_answered_correctly'].cumsum().shift(fill_value=0)\n",
    "    df['prior_relative_accuracy'] = (df['prior_relative_acc_sum'] / df['prior_quest_count']).fillna(0)\n",
    "    \n",
    "    # 각 문제 종류별(tag)로 이전에 몇번 풀었는지\n",
    "    # 1번 문제 3번 풀었다 / 3번 문제 1번 풀었다 ..etc\n",
    "    df['prior_tags_frequency'] = df.groupby(['userID', 'KnowledgeTag']).cumcount()\n",
    "\n",
    "    user_start_idx = df['userID'].diff() > 0\n",
    "    features = ['prior_acc_count',\n",
    "                'prior_quest_count',\n",
    "                'prior_acc',\n",
    "                'prior_relative_acc_sum',\n",
    "                'prior_relative_accuracy']\n",
    "    # 각 학생의 첫 row는 prior feature의 값을 0으로 초기화한다\n",
    "    df.loc[user_start_idx, features] = 0\n",
    "    \n",
    "    \n",
    "    # -- 바로전 feature 생성\n",
    "    # 각 시험지 문제별 마지막으로 푼 시간\n",
    "    prev_timestamp_ac = df.groupby(['userID', 'testId','day'])[['time_stamp']].shift()\n",
    "\n",
    "    # 각 문제 종류별 마지막으로 풀기 시작한 시점으로부터 지난 시간, elapsed_time\n",
    "    # 해당 문제 종류를 마지막으로 푼 시점으로부터 시간이 오래 지날수록 문제를 맞추기 힘들 것이다\n",
    "    df['diff_time_btw_item'] = (df['time_stamp'] - prev_timestamp_ac['time_stamp'])\n",
    "\n",
    "    # nan값은 [ diff_time_btw_content_ids ] 데이터 중 0 imputation을 한다\n",
    "    max_diff_time_btw_tags = df['diff_time_btw_item'].max()\n",
    "    df['diff_time_btw_item'] = df['diff_time_btw_item'].fillna(0)\n",
    "    \n",
    "    # 각 시험지 문제 종류별 마지막으로 풀었을때 정답 여부\n",
    "    prev_correct_ac = df.groupby(['userID', 'testId'])[['answerCode']].shift()        \n",
    "    df['prev_answered_correctly'] = prev_correct_ac['answerCode'].fillna(0)\n",
    "    \n",
    "    # 로그 스케일을 적용해야하는 데이터들은 어떤거지??\n",
    "    # 최소값이 0인 피처들은 로그 스케일을 하기 전에 +1을 해야할 듯 -> log1p가 그 역할을 함\n",
    "    log1p_cols = ['time_stamp',\n",
    "                  #'elapsed_time',\n",
    "                  'prior_tags_frequency',\n",
    "                  'diff_time_btw_item'\n",
    "                 ]\n",
    "\n",
    "    df[log1p_cols] = np.log1p(df[log1p_cols]+1)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e093f405-c38a-4c31-92df-b93247fb36f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = feature_engineering(df, total_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93bc4f87-336b-467f-bdfd-cca5193b66d8",
   "metadata": {},
   "source": [
    "### 수치형 feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a6bef50a-651c-4ad4-a878-f1ed39ae3b69",
   "metadata": {},
   "outputs": [],
   "source": [
    "cont_cols = ['time_stamp',\n",
    "            'diff_time_btw_item', \n",
    "            'prior_tags_frequency', \n",
    "            'prior_acc', \n",
    "            'acc_avg_by_test_id', \n",
    "            'acc_avg_by_assessment_item_id', \n",
    "            'acc_avg_by_tag', \n",
    "            #'lag_time', \n",
    "            #'prior_relative_acc_sum', 로그 스케일 적용 안됨\n",
    "            'prior_relative_accuracy', \n",
    "            'prev_answered_correctly', \n",
    "            #'relative_answered_correctly', # 로그 스케일 안됨\n",
    "            'answerCode']\n",
    "# train_df[cont_cols] = train_df[cont_cols].astype(np.float32)\n",
    "# val_df[cont_cols] = val_df[cont_cols].astype(np.float32) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9919fa36-45ad-491c-b66c-ed3ba7812331",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "import time\n",
    "import tqdm\n",
    "import pandas as pd\n",
    "import random\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "824bbae3-fc41-4fa3-be82-40eb92b8d784",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_labels(encoder, name):\n",
    "    le_path = os.path.join('./asset', name + '_classes.npy')\n",
    "    np.save(le_path, encoder.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "eda69b9c-aba5-4923-ba93-cf7930948020",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(df, is_train = True):\n",
    "    cate_cols = ['assessmentItemID', 'testId', 'KnowledgeTag']\n",
    "    \n",
    "    asset_dir = './asset'\n",
    "    if not os.path.exists(asset_dir):\n",
    "        os.makedirs(asset_dir)\n",
    "\n",
    "    for col in cate_cols:\n",
    "        le = LabelEncoder()\n",
    "        if is_train:\n",
    "            #For UNKNOWN class\n",
    "            a = df[col].unique().tolist() + ['unknown']\n",
    "            le.fit(a)\n",
    "            save_labels(le, col)\n",
    "        else:\n",
    "            label_path = os.path.join(asset_dir,col+'_classes.npy')\n",
    "            le.classes_ = np.load(label_path)\n",
    "\n",
    "            df[col] = df[col].apply(lambda x: x if x in le.classes_ else 'unknown')\n",
    "\n",
    "        #모든 컬럼이 범주형이라고 가정\n",
    "        df[col] = df[col].astype(str)\n",
    "        test = le.transform(df[col])\n",
    "        df[col] = test\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "00ad0749-da8e-4297-aa0a-6637ff430df4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.sort_values(by=['userID','Timestamp'], axis=0)\n",
    "columns = ['userID', 'assessmentItemID', 'testId', 'answerCode', 'KnowledgeTag']\n",
    "cont_cols = ['time_stamp',\n",
    "            'diff_time_btw_item', \n",
    "            'prior_tags_frequency', \n",
    "            'prior_acc', \n",
    "            'acc_avg_by_test_id', \n",
    "            'acc_avg_by_assessment_item_id', \n",
    "            'acc_avg_by_tag', \n",
    "            #'lag_time', \n",
    "            #'prior_relative_acc_sum', 로그 스케일 적용 안됨\n",
    "            'prior_relative_accuracy', \n",
    "            'prev_answered_correctly'\n",
    "            #'relative_answered_correctly', # 로그 스케일 안됨\n",
    "            ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9272ec4a-4c26-4f1d-b190-159859590008",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_size = 5358, val_size = 1340\n",
      "test set 과 같은 문제를 예측하는 user 수 = 4366\n",
      "[6329, 6370, 328, 7030, 6268]\n",
      "[0, 1, 2, 5, 6]\n",
      "train_ids_len = 1818587\n",
      "val_ids_len = 447999\n"
     ]
    }
   ],
   "source": [
    "# 여기서 train, val 나눔\n",
    "train_ids, val_ids = get_train_val_row_ids(df, test_df)\n",
    "df = preprocessing(df, is_train=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0078d579-aa31-478b-99d6-f4aca4eb6168",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = df.loc[train_ids].reset_index()\n",
    "val_df = df.loc[val_ids].reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d8ef4b85-860e-4137-97ec-03833f842905",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5358\n",
      "1340\n"
     ]
    }
   ],
   "source": [
    "print(len(train_df.groupby('userID')))\n",
    "print(len(val_df.groupby('userID')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c10f0913-3f18-4730-acc6-48e52572849b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_group = train_df[columns+cont_cols].groupby('userID').apply(\n",
    "        lambda r: (\n",
    "            r['testId'].values, \n",
    "            r['assessmentItemID'].values,\n",
    "            r['KnowledgeTag'].values,\n",
    "            r['answerCode'].values,\n",
    "            r['time_stamp'].values,\n",
    "            r['diff_time_btw_item'].values,\n",
    "            r['prior_tags_frequency'].values,\n",
    "            r['prior_acc'].values,\n",
    "            r['acc_avg_by_test_id'].values,\n",
    "            r['acc_avg_by_assessment_item_id'].values,\n",
    "            r['acc_avg_by_tag'].values,\n",
    "            r['prior_relative_accuracy'].values,\n",
    "            r['prev_answered_correctly'].values,\n",
    "            \n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "885f9206-11fd-46f5-a8af-6c72903d51da",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_group = val_df[columns+cont_cols].groupby('userID').apply(\n",
    "        lambda r: (\n",
    "            r['testId'].values, \n",
    "            r['assessmentItemID'].values,\n",
    "            r['KnowledgeTag'].values,\n",
    "            r['answerCode'].values,\n",
    "            r['time_stamp'].values,\n",
    "            r['diff_time_btw_item'].values,\n",
    "            r['prior_tags_frequency'].values,\n",
    "            r['prior_acc'].values,\n",
    "            r['acc_avg_by_test_id'].values,\n",
    "            r['acc_avg_by_assessment_item_id'].values,\n",
    "            r['acc_avg_by_tag'].values,\n",
    "            r['prior_relative_accuracy'].values,\n",
    "            r['prev_answered_correctly'].values,\n",
    "            \n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cfd8025b-6484-477c-8c95-79e790236661",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train_group.values\n",
    "valid_data = val_group.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "58024d15-417c-467e-bfcb-cd3947353c36",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('train_data.txt', 'wb') as f:\n",
    "    pickle.dump(train_data, f)\n",
    "with open('valid_data.txt', 'wb') as f:\n",
    "    pickle.dump(valid_data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f268930c-91f9-4dd5-9467-a81a57927eb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDKTDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data, args):\n",
    "        self.data = data\n",
    "        self.args = args\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        row = self.data[index]\n",
    "\n",
    "        # 각 data의 sequence length\n",
    "        seq_len = len(row[0])\n",
    "\n",
    "        test, question, tag, correct = row[0], row[1], row[2], row[3]\n",
    "        cont1,cont2,cont3,cont4,cont5,cont6,cont7,cont8,cont9 = row[4], row[5], row[6], row[7],row[8], row[9], row[10], row[11],row[12]\n",
    "        \n",
    "        cont_cols = [cont1,cont2,cont3,cont4,cont5,cont6,cont7,cont8,cont9]\n",
    "        cate_cols = [test, question, tag, correct]\n",
    "\n",
    "        # max seq len을 고려하여서 이보다 길면 자르고 아닐 경우 그대로 냅둔다\n",
    "        if seq_len > self.args.max_seq_len:\n",
    "            for i, col in enumerate(cate_cols):\n",
    "                cate_cols[i] = col[-self.args.max_seq_len:]\n",
    "            mask = np.ones(self.args.max_seq_len, dtype=np.int16)\n",
    "            \n",
    "            for i, col in enumerate(cont_cols):\n",
    "                cont_cols[i] = col[-self.args.max_seq_len:]\n",
    "        else:\n",
    "            mask = np.zeros(self.args.max_seq_len, dtype=np.int16)\n",
    "            mask[-seq_len:] = 1\n",
    "\n",
    "        # mask도 columns 목록에 포함시킴\n",
    "        cate_cols.append(mask)\n",
    "\n",
    "        # np.array -> torch.tensor 형변환\n",
    "        for i, col in enumerate(cate_cols):\n",
    "            cate_cols[i] = torch.tensor(col)\n",
    "        \n",
    "        for i, col in enumerate(cont_cols):\n",
    "            cont_cols[i] = torch.tensor(col)\n",
    "\n",
    "        return cate_cols+cont_cols\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcb8939c-bcd8-4891-bc78-535dfd3995af",
   "metadata": {},
   "source": [
    "train_loader, valid_loader = get_loaders(args, train_data, valid_data)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "28d32462-8bdf-40b8-a8e8-37ed51432f84",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate(batch):\n",
    "    col_n = len(batch[0])\n",
    "    col_list = [[] for _ in range(col_n)]\n",
    "    max_seq_len = len(batch[0][-1])\n",
    "\n",
    "    # batch의 값들을 각 column끼리 그룹화\n",
    "    for row in batch:\n",
    "        for i, col in enumerate(row):\n",
    "            pre_padded = torch.zeros(max_seq_len)\n",
    "            pre_padded[-len(col):] = col  # 앞부분이 0으로 padding됨\n",
    "            col_list[i].append(pre_padded)\n",
    "\n",
    "\n",
    "    for i, _ in enumerate(col_list):\n",
    "        col_list[i] =torch.stack(col_list[i])\n",
    "    \n",
    "    return tuple(col_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7598aeb7-405e-4028-b2b8-ef662c9d6186",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "#from args import parse_args\n",
    "import argparse\n",
    "from dkt.dataloader import Preprocess\n",
    "from dkt import trainer\n",
    "import torch\n",
    "from dkt.utils import setSeeds\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a446ad90-709a-4931-9245-a67bac56fe2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_args(mode='train'):\n",
    "    parser = argparse.ArgumentParser()\n",
    "\n",
    "    \n",
    "    parser.add_argument('--seed', default=42, type=int, help='seed')\n",
    "    \n",
    "    parser.add_argument('--device', default='cpu', type=str, help='cpu or gpu')\n",
    "\n",
    "    parser.add_argument('--data_dir', default='/opt/ml/input/data/train_dataset', type=str, help='data directory')\n",
    "    parser.add_argument('--asset_dir', default='asset/', type=str, help='data directory')\n",
    "    \n",
    "    parser.add_argument('--file_name', default='train_data.csv', type=str, help='train file name')\n",
    "    \n",
    "    parser.add_argument('--model_dir', default='models/', type=str, help='model directory')\n",
    "    parser.add_argument('--model_name', default='model.pt', type=str, help='model file name')\n",
    "\n",
    "    parser.add_argument('--output_dir', default='output/', type=str, help='output directory')\n",
    "    parser.add_argument('--test_file_name', default='test_data.csv', type=str, help='test file name')\n",
    "    \n",
    "    parser.add_argument('--max_seq_len', default=20, type=int, help='max sequence length')\n",
    "    parser.add_argument('--num_workers', default=4, type=int, help='number of workers')\n",
    "\n",
    "    # 모델\n",
    "    parser.add_argument('--hidden_dim', default=64, type=int, help='hidden dimension size')\n",
    "    parser.add_argument('--n_layers', default=2, type=int, help='number of layers')\n",
    "    parser.add_argument('--n_heads', default=2, type=int, help='number of heads')\n",
    "    parser.add_argument('--drop_out', default=0.2, type=float, help='drop out rate')\n",
    "    \n",
    "    # 훈련\n",
    "    parser.add_argument('--n_epochs', default=20, type=int, help='number of epochs')\n",
    "    parser.add_argument('--batch_size', default=64, type=int, help='batch size')\n",
    "    parser.add_argument('--lr', default=0.0001, type=float, help='learning rate')\n",
    "    parser.add_argument('--clip_grad', default=10, type=int, help='clip grad')\n",
    "    parser.add_argument('--patience', default=5, type=int, help='for early stopping')\n",
    "    \n",
    "\n",
    "    parser.add_argument('--log_steps', default=50, type=int, help='print log per n steps')\n",
    "    \n",
    "\n",
    "    ### 중요 ###\n",
    "    parser.add_argument('--model', default='lstm', type=str, help='model type')\n",
    "    parser.add_argument('--optimizer', default='adam', type=str, help='optimizer type')\n",
    "    parser.add_argument('--scheduler', default='plateau', type=str, help='scheduler type')\n",
    "    \n",
    "    args = parser.parse_args([])\n",
    "\n",
    "    return args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a6cbf8de-3af2-46d9-be96-d7c88c4e6939",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "args = parse_args(mode='train')\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "args.device = device\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "45f1ab67-563b-41e0-8fde-c47e15df6f8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 배치 전처리\n",
    "def process_batch(batch, args):\n",
    "    #print('<<<<< process_batch >>>')\n",
    "\n",
    "    test, question, tag, correct, mask = batch[:5]   # [batch_size(64), max_seq_len(20)]\n",
    "    cont = batch[5:]\n",
    "    \n",
    "    # change to float\n",
    "    mask = mask.type(torch.FloatTensor)\n",
    "    correct = correct.type(torch.FloatTensor)\n",
    "\n",
    "    # interaction: 과거 정답 여부를 다음 시퀀스에 추가적인 feature로 사용하게끔 한칸 시프트 해준 feature\n",
    "    #  interaction을 임시적으로 correct를 한칸 우측으로 이동한 것으로 사용\n",
    "    #    saint의 경우 decoder에 들어가는 input이다\n",
    "    interaction = correct + 1 # 패딩을 위해 correct값에 1을 더해준다. (정답 2, 오답 1)\n",
    "    interaction = interaction.roll(shifts=1, dims=1)\n",
    "    interaction_mask = mask.roll(shifts=1, dims=1)\n",
    "    interaction_mask[:, 0] = 0\n",
    "    interaction = (interaction * interaction_mask).to(torch.int64)  # 가장 마지막으로 푼 문제를 제외하고 정답 2, 오답 1\n",
    "    # print(interaction)\n",
    "    # exit()\n",
    "    #  test_id, question_id, tag\n",
    "    test = ((test + 1) * mask).to(torch.int64)\n",
    "    question = ((question + 1) * mask).to(torch.int64)\n",
    "    tag = ((tag + 1) * mask).to(torch.int64)\n",
    "\n",
    "\n",
    "    # device memory로 이동\n",
    "\n",
    "    test = test.to(args.device)\n",
    "    question = question.to(args.device)\n",
    "\n",
    "\n",
    "    tag = tag.to(args.device)\n",
    "    correct = correct.to(args.device)\n",
    "    mask = mask.to(args.device)\n",
    "\n",
    "    interaction = interaction.to(args.device)\n",
    "    \n",
    "    cont_features = torch.cat(cont, 1).view(args.batch_size, args.max_seq_len,-1)\n",
    "    cont_features = cont_features.to(args.device)\n",
    "\n",
    "    return (test, question,\n",
    "            tag, correct, mask,\n",
    "            interaction, cont_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d2d3a56d-9a9a-43e5-924f-dae807d38b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F \n",
    "import numpy as np\n",
    "import copy\n",
    "import math\n",
    "\n",
    "try:\n",
    "    from transformers.modeling_bert import BertConfig, BertEncoder, BertModel    \n",
    "except:\n",
    "    from transformers.models.bert.modeling_bert import BertConfig, BertEncoder, BertModel    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "73410c97-07e3-4ec1-8c6c-31442aa66ce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 추후 feature를 embedding할 시에 embedding_layer의 input 크기를 결정할때 사용\n",
    "args.n_questions = len(np.load(os.path.join(args.asset_dir,'assessmentItemID_classes.npy')))\n",
    "args.n_test = len(np.load(os.path.join(args.asset_dir,'testId_classes.npy')))\n",
    "args.n_tag = len(np.load(os.path.join(args.asset_dir,'KnowledgeTag_classes.npy')))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ba83825-1ce7-478a-950e-fa5c987123df",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "\n",
    "    def __init__(self, args):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.args = args\n",
    "        self.device = args.device\n",
    "\n",
    "        self.hidden_dim = self.args.hidden_dim\n",
    "        self.n_layers = self.args.n_layers\n",
    "        \n",
    "        self.cont_col_size = 8\n",
    "\n",
    "        # Embedding \n",
    "        # interaction은 현재 correct로 구성되어있다. correct(1, 2) + padding(0)\n",
    "        self.embedding_interaction = nn.Embedding(3, self.hidden_dim//3)\n",
    "        self.embedding_test = nn.Embedding(self.args.n_test + 1, self.hidden_dim//3)\n",
    "        self.embedding_question = nn.Embedding(self.args.n_questions + 1, self.hidden_dim//3)\n",
    "        self.embedding_tag = nn.Embedding(self.args.n_tag + 1, self.hidden_dim//3)\n",
    "\n",
    "\n",
    "        # embedding combination projection\n",
    "        self.cate_proj =  nn.Sequential(\n",
    "            nn.Linear((self.hidden_dim//3)*4, self.hidden_dim),\n",
    "            nn.LayerNorm(self.hidden_dim),\n",
    "        )\n",
    "        \n",
    "        # continuous\n",
    "        self.cont_bn = nn.BatchNorm1d(self.cont_col_size)\n",
    "        self.cont_emb = nn.Sequential(\n",
    "            nn.Linear(self.cont_col_size, self.hidden_dim),\n",
    "            nn.LayerNorm(self.hidden_dim),\n",
    "        )\n",
    "        \n",
    "        # combination\n",
    "        self.comb_proj = nn.Sequential(\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(self.hidden_dim*2, self.hidden_dim),\n",
    "            nn.LayerNorm(self.hidden_dim),\n",
    "        )\n",
    "\n",
    "\n",
    "        self.lstm = nn.LSTM(self.hidden_dim,\n",
    "                            self.hidden_dim,\n",
    "                            self.n_layers,\n",
    "                            batch_first=True)\n",
    "        \n",
    "        # Fully connected layer\n",
    "        self.fc = nn.Linear(self.hidden_dim, 1)\n",
    "\n",
    "        self.activation = nn.Sigmoid()\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        h = torch.zeros(\n",
    "            self.n_layers,\n",
    "            batch_size,\n",
    "            self.hidden_dim)\n",
    "        h = h.to(self.device)\n",
    "\n",
    "        c = torch.zeros(\n",
    "            self.n_layers,\n",
    "            batch_size,\n",
    "            self.hidden_dim)\n",
    "        c = c.to(self.device)\n",
    "\n",
    "        return (h, c)\n",
    "\n",
    "    def forward(self, input):\n",
    "\n",
    "        test, question, tag, _, mask, interaction, cont_x = input\n",
    "        self.cont_col_size = cont_x.size(-1)\n",
    "\n",
    "        batch_size = interaction.size(0)\n",
    "\n",
    "        # Embedding\n",
    "\n",
    "        embed_interaction = self.embedding_interaction(interaction)\n",
    "        embed_test = self.embedding_test(test)\n",
    "        embed_question = self.embedding_question(question)\n",
    "        embed_tag = self.embedding_tag(tag)\n",
    "        \n",
    "\n",
    "        cate_emb = torch.cat([embed_interaction,\n",
    "                           embed_test,\n",
    "                           embed_question,\n",
    "                           embed_tag,], 2)\n",
    "        \n",
    "        cate_emb = self.cate_proj(cate_emb)\n",
    "        \n",
    "        # continuous\n",
    "        cont_x = self.cont_bn(cont_x.view(-1, cont_x.size(-1))).view(batch_size, -1, cont_x.size(-1))\n",
    "        cont_emb = self.cont_emb(cont_x.view(batch_size, args.max_seq_len, -1))        \n",
    "        \n",
    "        # combination\n",
    "        seq_emb = torch.cat([cate_emb, cont_emb], 2)        \n",
    "        X = self.comb_proj(seq_emb)   \n",
    "        #X = self.comb_proj(embed)\n",
    "\n",
    "        hidden = self.init_hidden(batch_size)\n",
    "        out, hidden = self.lstm(X, hidden)\n",
    "        out = out.contiguous().view(batch_size, -1, self.hidden_dim)  # .contiguous(): 새로운 텐서를 반환\n",
    "\n",
    "        out = self.fc(out)\n",
    "        preds = self.activation(out).view(batch_size, -1)\n",
    "\n",
    "        return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b61f0d75-a63d-4c60-92e3-f3e86ca8f5e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "#from dkt.dataloader import get_loaders\n",
    "from dkt.optimizer import get_optimizer\n",
    "from dkt.scheduler import get_scheduler\n",
    "from dkt.criterion import get_criterion\n",
    "from dkt.metric import get_metric\n",
    "from dkt.model import LSTM, LSTMATTN, Bert, Saint\n",
    "\n",
    "from dkt.trainer import get_lr, inference, get_model, compute_loss, update_params, save_checkpoint, load_model\n",
    "\n",
    "import wandb\n",
    "import time\n",
    "import datetime\n",
    "import gc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2a73efe6-1ab1-41c5-8403-80a0b924e475",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_DIR = 'models/'\n",
    "os.makedirs(MODEL_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "21207e7a-cf44-46af-b496-25374517059f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loaders(args, train, valid):\n",
    "\n",
    "    pin_memory = True  # False\n",
    "    train_loader, valid_loader = None, None\n",
    "    \n",
    "    if train is not None:\n",
    "        trainset = CustomDKTDataset(train, args)\n",
    "        train_loader = torch.utils.data.DataLoader(trainset, num_workers=args.num_workers, shuffle=True,\n",
    "                            batch_size=args.batch_size, pin_memory=pin_memory, collate_fn=collate)\n",
    "    if valid is not None:\n",
    "        valset = CustomDKTDataset(valid, args)\n",
    "        valid_loader = torch.utils.data.DataLoader(valset, num_workers=args.num_workers, shuffle=False,\n",
    "                            batch_size=args.batch_size, pin_memory=pin_memory, collate_fn=collate)\n",
    "\n",
    "    return train_loader, valid_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "8a6c47d9-3622-47db-b07b-7f35e30f0629",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train(train_loader, model, optimizer, args):\n",
    "    model.train()\n",
    "\n",
    "    total_preds = []\n",
    "    total_targets = []\n",
    "    losses = []\n",
    "    for step, batch in enumerate(train_loader):\n",
    "        input = process_batch(batch, args)\n",
    "        preds = model(input)\n",
    "        targets = input[3] # correct\n",
    "\n",
    "\n",
    "        loss = compute_loss(preds, targets)\n",
    "        update_params(loss, model, optimizer, args)\n",
    "\n",
    "\n",
    "        if step % args.log_steps == 0:\n",
    "            print(f\"Training steps: {step} Loss: {str(loss.item())}\")\n",
    "        \n",
    "        # predictions\n",
    "        preds = preds[:,-1]\n",
    "        targets = targets[:,-1]\n",
    "\n",
    "        if args.device == 'cuda':\n",
    "            preds = preds.to('cpu').detach().numpy()\n",
    "            targets = targets.to('cpu').detach().numpy()\n",
    "        else: # cpu\n",
    "            preds = preds.detach().numpy()\n",
    "            targets = targets.detach().numpy()\n",
    "        \n",
    "        total_preds.append(preds)\n",
    "        total_targets.append(targets)\n",
    "        losses.append(loss)\n",
    "      \n",
    "\n",
    "    total_preds = np.concatenate(total_preds)\n",
    "    total_targets = np.concatenate(total_targets)\n",
    "\n",
    "    # Train AUC / ACC\n",
    "    auc, acc = get_metric(total_targets, total_preds)\n",
    "    loss_avg = sum(losses)/len(losses)\n",
    "    print(f'TRAIN AUC : {auc} ACC : {acc}')\n",
    "    return auc, acc, loss_avg\n",
    "    \n",
    "\n",
    "def validate(valid_loader, model, args):\n",
    "    model.eval()\n",
    "\n",
    "    total_preds = []\n",
    "    total_targets = []\n",
    "    for step, batch in enumerate(valid_loader):\n",
    "        input = process_batch(batch, args)\n",
    "\n",
    "        preds = model(input)\n",
    "        targets = input[3] # correct\n",
    "\n",
    "\n",
    "        # predictions\n",
    "        preds = preds[:,-1]\n",
    "        targets = targets[:,-1]\n",
    "    \n",
    "        if args.device == 'cuda':\n",
    "            preds = preds.to('cpu').detach().numpy()\n",
    "            targets = targets.to('cpu').detach().numpy()\n",
    "        else: # cpu\n",
    "            preds = preds.detach().numpy()\n",
    "            targets = targets.detach().numpy()\n",
    "\n",
    "        total_preds.append(preds)\n",
    "        total_targets.append(targets)\n",
    "\n",
    "    total_preds = np.concatenate(total_preds)\n",
    "    total_targets = np.concatenate(total_targets)\n",
    "\n",
    "    # Train AUC / ACC\n",
    "    auc, acc = get_metric(total_targets, total_preds)\n",
    "    \n",
    "    print(f'VALID AUC : {auc} ACC : {acc}\\n')\n",
    "\n",
    "    return auc, acc, total_preds, total_targets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "bbba0cd4-c0b1-4366-9584-aef7f4fc19eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(args, train_data, valid_data):\n",
    "    train_loader, valid_loader = get_loaders(args, train_data, valid_data)\n",
    "    \n",
    "    # only when using warmup scheduler\n",
    "    args.total_steps = int(len(train_loader.dataset) / args.batch_size) * (args.n_epochs)\n",
    "    args.warmup_steps = args.total_steps // 10\n",
    "            \n",
    "    #model = get_model(args)\n",
    "    model = LSTM(args)\n",
    "    model.to(args.device)\n",
    "    \n",
    "    optimizer = get_optimizer(model, args)\n",
    "    scheduler = get_scheduler(optimizer, args)\n",
    "\n",
    "    best_auc = -1\n",
    "    early_stopping_counter = 0\n",
    "    for epoch in range(args.n_epochs):\n",
    "\n",
    "        print(f\"Start Training: Epoch {epoch + 1}\")\n",
    "        start = time.time()\n",
    "        ### TRAIN\n",
    "        train_auc, train_acc, train_loss = train(train_loader, model, optimizer, args)\n",
    "        \n",
    "        ### VALID\n",
    "        auc, acc,_ , _ = validate(valid_loader, model, args)\n",
    "\n",
    "        sec = time.time() - start\n",
    "        times = str(datetime.timedelta(seconds=sec)).split(\".\")\n",
    "        times = times[0]\n",
    "        print(f'<<<<<<<<<<  {epoch + 1} EPOCH spent : {times}  >>>>>>>>>>')\n",
    "\n",
    "        ### TODO: model save or early stopping\n",
    "        wandb.log({\"epoch\": epoch, \"train_loss\": train_loss, \"train_auc\": train_auc, \"train_acc\":train_acc,\n",
    "                  \"valid_auc\":auc, \"valid_acc\":acc, \"Learning_rate\": get_lr(optimizer),})\n",
    "        \n",
    "        if auc > best_auc:\n",
    "            best_auc = auc\n",
    "            # torch.nn.DataParallel로 감싸진 경우 원래의 model을 가져옵니다.\n",
    "            model_to_save = model.module if hasattr(model, 'module') else model\n",
    "            save_checkpoint({\n",
    "                'epoch': epoch + 1,\n",
    "                'state_dict': model_to_save.state_dict(),\n",
    "                },\n",
    "                args.model_dir, 'model.pt',\n",
    "            )\n",
    "            early_stopping_counter = 0\n",
    "        else:\n",
    "            early_stopping_counter += 1\n",
    "            if early_stopping_counter >= args.patience:\n",
    "                print(f'EarlyStopping counter: {early_stopping_counter} out of {args.patience}')\n",
    "                break\n",
    "\n",
    "        # scheduler\n",
    "        if args.scheduler == 'plateau':\n",
    "            scheduler.step(best_auc)\n",
    "        else:\n",
    "            scheduler.step()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "3b12f7d9-3445-4c55-9acf-5771e69ffc6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mdhh0\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.10.31 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.10.30<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">lstm</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/dhh0/dkt\" target=\"_blank\">https://wandb.ai/dhh0/dkt</a><br/>\n",
       "                Run page: <a href=\"https://wandb.ai/dhh0/dkt/runs/1cdxnu0e\" target=\"_blank\">https://wandb.ai/dhh0/dkt/runs/1cdxnu0e</a><br/>\n",
       "                Run data is saved locally in <code>/opt/ml/my_code/wandb/run-20210607_055821-1cdxnu0e</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h1>Run(1cdxnu0e)</h1><iframe src=\"https://wandb.ai/dhh0/dkt/runs/1cdxnu0e\" style=\"border:none;width:100%;height:400px\"></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7fedcaa0d750>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.init(project='dkt', config=vars(args), tags=['lstm'], name='lstm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "26ba037e-c269-4440-8fcd-7a2c5453e495",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Training: Epoch 1\n",
      "torch.Size([64, 180])\n",
      "torch.Size([64, 20, 9])\n",
      "Training steps: 0 Loss: 0.6896703243255615\n",
      "torch.Size([64, 180])\n",
      "torch.Size([64, 20, 9])\n",
      "torch.Size([64, 180])\n",
      "torch.Size([64, 20, 9])\n",
      "torch.Size([64, 180])\n",
      "torch.Size([64, 20, 9])\n",
      "torch.Size([64, 180])\n",
      "torch.Size([64, 20, 9])\n",
      "torch.Size([64, 180])\n",
      "torch.Size([64, 20, 9])\n",
      "torch.Size([64, 180])\n",
      "torch.Size([64, 20, 9])\n",
      "torch.Size([64, 180])\n",
      "torch.Size([64, 20, 9])\n",
      "torch.Size([64, 180])\n",
      "torch.Size([64, 20, 9])\n",
      "torch.Size([64, 180])\n",
      "torch.Size([64, 20, 9])\n",
      "torch.Size([64, 180])\n",
      "torch.Size([64, 20, 9])\n",
      "torch.Size([64, 180])\n",
      "torch.Size([64, 20, 9])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Caught RuntimeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/opt/conda/lib/python3.7/site-packages/torch/utils/data/_utils/worker.py\", line 185, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/opt/conda/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\", line 47, in fetch\n    return self.collate_fn(data)\n  File \"<ipython-input-27-9ff0f4f30542>\", line 17, in collate\n    pre_padded[-len(col):] = col  # 앞부분이 0으로 padding됨\nRuntimeError: The expanded size of the tensor (17) must match the existing size (20) at non-singleton dimension 0.  Target sizes: [17].  Tensor sizes: [20]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-41-3d179a1b5b11>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-39-d54f2341ec5a>\u001b[0m in \u001b[0;36mrun\u001b[0;34m(args, train_data, valid_data)\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0;31m### TRAIN\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m         \u001b[0mtrain_auc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_acc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0;31m### VALID\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-38-46633a75ff7d>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(train_loader, model, optimizer, args)\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mtotal_targets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mlosses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m         \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocess_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    361\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    362\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 363\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    364\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    365\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    987\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    988\u001b[0m                 \u001b[0;32mdel\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_task_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 989\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    990\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    991\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_try_put_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_process_data\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   1012\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_put_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1013\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mExceptionWrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1014\u001b[0;31m             \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1015\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1016\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/_utils.py\u001b[0m in \u001b[0;36mreraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    393\u001b[0m             \u001b[0;31m# (https://bugs.python.org/issue2651), so we work around it.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    394\u001b[0m             \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mKeyErrorMessage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 395\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m: Caught RuntimeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/opt/conda/lib/python3.7/site-packages/torch/utils/data/_utils/worker.py\", line 185, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/opt/conda/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\", line 47, in fetch\n    return self.collate_fn(data)\n  File \"<ipython-input-27-9ff0f4f30542>\", line 17, in collate\n    pre_padded[-len(col):] = col  # 앞부분이 0으로 padding됨\nRuntimeError: The expanded size of the tensor (17) must match the existing size (20) at non-singleton dimension 0.  Target sizes: [17].  Tensor sizes: [20]\n"
     ]
    }
   ],
   "source": [
    "run(args, train_data, valid_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "3eac5dc9-c5e7-4a33-b621-c9cc429c5eb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "tt = torch.rand([3,11])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "3fabc7ab-8776-4fb6-80ed-4d4828a32bc9",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "shape '[3, 5, -1]' is invalid for input of size 33",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-51-5d63cbcc45f1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m: shape '[3, 5, -1]' is invalid for input of size 33"
     ]
    }
   ],
   "source": [
    "tt.unsqueeze(-1).view([3,5,-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66a96d64-ac46-4ff0-b7c0-aa3047ba4563",
   "metadata": {},
   "source": [
    "## preds = model(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "d8ae1bcb-80ba-4ecd-9f0d-b7c8d4985b8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 20])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "557d6434-df90-4287-8815-964b7ded6fb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 20, 8])\n",
      "8\n"
     ]
    }
   ],
   "source": [
    "cont_features = input[6]\n",
    "print(cont_features.shape)\n",
    "print(cont_features.size(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "978843bf-45f9-4259-ac16-371893f35be0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# continuous\n",
    "cont_col_size = cont_features.size(-1)\n",
    "print(cont_col_size)\n",
    "cont_bn = nn.BatchNorm1d(cont_col_size)\n",
    "cont_bn.to(args.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "0d2e5ba2-a947-4387-89f7-805620058d76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1280, 8])"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# batchnorm 1d 적용\n",
    "cont_bn_x = cont_bn(cont_features.view(-1, cont_features.size(-1)))\n",
    "cont_bn_x.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "3d265e71-b588-47e2-b3f0-d743dd1583f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 20, 8])"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# batchnorm 적용 이후 원래 사이즈 복구\n",
    "cont_bn_x = cont_bn_x.view(args.batch_size, -1, cont_features.size(-1))\n",
    "cont_bn_x.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "2cd3da81-75ec-4b86-9dde-5266026c7759",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 범주형과는 다르게 embedding없이 바로 projection을 통해 원하는 사이즈로 줄인다\n",
    "# 여기서는 embedding이라고 부른다\n",
    "# [16, 16, 36] -> [16, 16, 128]\n",
    "lin = nn.Linear(cont_col_size, args.hidden_dim)\n",
    "ln =  nn.LayerNorm(args.hidden_dim)\n",
    "# cont_emb = nn.Sequential(nn.Linear(cont_col_size, args.hidden_dim),\n",
    "#                          nn.LayerNorm(args.hidden_dim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "212e8f49-9312-4236-87f2-fd60a942755b",
   "metadata": {},
   "outputs": [],
   "source": [
    "lin.to(args.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "f5d56442-4ff9-4409-93b8-570aa6a72936",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args.hidden_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1cbd8ac-5cb2-40a8-907c-ede46a30978c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd83b256-604c-4ab8-a7c0-87faadeeac72",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a2e9d59-79c2-46b1-9058-04ac95b76e82",
   "metadata": {},
   "outputs": [],
   "source": [
    "cont_emb.to(args.device)\n",
    "cont_embed_x = cont_emb(cont_bn_x)\n",
    "cont_embed_x.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "091688c9-0fe1-471d-87ae-c71d38e4b47a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ec6af5c-968b-48e0-bb1a-2e71ec55f14f",
   "metadata": {},
   "outputs": [],
   "source": [
    "cont_emb = nn.Sequential(\n",
    "    nn.Linear(cont_col_size*cfg.n_rows_per_step, self.hidden_dim),\n",
    "    nn.LayerNorm(self.hidden_dim),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "239d6ff5-7bd6-43b8-a16d-9de3dcb995e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8551c8db-a0d4-48f3-8194-326e01d4bf8b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e49aea95-1cd9-44df-989a-0b1a29af183a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5237eb6-eddd-4d0f-b34e-0716bbac8751",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a249a6af-93f0-4dd4-bc74-868fa1c72590",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f8c54b49-672a-42b3-8cf6-6482e52261ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "afdfdb26-2a48-419a-8393-32add8517c8c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "50f76043-4961-4970-9231-01bebf6c2007",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "566735f0-8515-4712-92ec-e20bb1dbbd77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a98607bd-2735-4f36-afe7-c0c967929eda",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_from_file(self, file_name, is_train=True):\n",
    "    csv_file_path = os.path.join(self.args.data_dir, file_name)\n",
    "    df = pd.read_csv(csv_file_path)#, nrows=100000)\n",
    "    df = self.__feature_engineering(df)\n",
    "    df = self.__preprocessing(df, is_train)\n",
    "\n",
    "    # 추후 feature를 embedding할 시에 embedding_layer의 input 크기를 결정할때 사용\n",
    "    self.args.n_questions = len(np.load(os.path.join(self.args.asset_dir,'assessmentItemID_classes.npy')))\n",
    "    self.args.n_test = len(np.load(os.path.join(self.args.asset_dir,'testId_classes.npy')))\n",
    "    self.args.n_tag = len(np.load(os.path.join(self.args.asset_dir,'KnowledgeTag_classes.npy')))\n",
    "\n",
    "\n",
    "\n",
    "    df = df.sort_values(by=['userID','Timestamp'], axis=0)\n",
    "    columns = ['userID', 'assessmentItemID', 'testId', 'answerCode', 'KnowledgeTag']\n",
    "    group = df[columns].groupby('userID').apply(\n",
    "            lambda r: (\n",
    "                r['testId'].values, \n",
    "                r['assessmentItemID'].values,\n",
    "                r['KnowledgeTag'].values,\n",
    "                r['answerCode'].values\n",
    "            )\n",
    "        )\n",
    "\n",
    "    return group.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcaf068b-16eb-4995-bea1-87b90b9bac06",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess = Preprocess(args)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "76db3cf4-2661-4619-ba70-ccce834bc048",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "import time\n",
    "from datetime import datetime\n",
    "import random\n",
    "\n",
    "import pickle\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import os, random, torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "448c3667-7548-4149-8716-e6cc63f8b5fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setSeeds(seed = 42):\n",
    "    # 랜덤 시드를 설정하여 매 코드를 실행할 때마다 동일한 결과를 얻게 합니다.\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)    \n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "47b51347-ffb2-49ea-90e5-67ef180d08e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "setSeeds(seed = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8e7a0901-9567-4648-b288-3d1378a4d027",
   "metadata": {},
   "outputs": [],
   "source": [
    "dtype = {\n",
    "    'userID': 'int16',\n",
    "    'answerCode': 'int8',\n",
    "    'KnowledgeTag': 'int16'\n",
    "}   \n",
    "\n",
    "# 데이터 경로 \n",
    "train_path = '../input/data/train_dataset/train_data.csv'\n",
    "test_path = '../input/data/train_dataset/test_data.csv'\n",
    "\n",
    "df = pd.read_csv(train_path)\n",
    "df = df.sort_values(by=['userID', 'Timestamp']).reset_index(drop=True)\n",
    "test_df = pd.read_csv(test_path)\n",
    "total_df = pd.concat([df, test_df[test_df.answerCode!=-1]], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a002b4c0-d67c-4111-b217-381112d3a1e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_time(s):\n",
    "    timestamp = time.mktime(datetime.strptime(s, '%Y-%m-%d %H:%M:%S').timetuple())\n",
    "    return int(timestamp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cc4c0fe0-114a-4ed9-b7d8-87917d08c547",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_elapsed_time(df):\n",
    "    prev_timestamp = df.groupby(['userID','testId','day'])[['time_stamp']].shift()\n",
    "    df['elapsed_time'] = df['time_stamp'] - prev_timestamp['time_stamp']\n",
    "    df['elapsed_time'] = df['elapsed_time'].fillna(0)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4b8cab38-c151-4071-b77b-65c124a628f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_lag_time(df):\n",
    "    start_end_id_by_user_test = df.groupby(['userID','testId','day']).apply(lambda x: (x.index.values[0], x.index.values[-1])).reset_index()\n",
    "    start_end_id_by_user_test = start_end_id_by_user_test.sort_values(by=[0]).reset_index(drop=True)\n",
    "    start_row_id_by_user = start_end_id_by_user_test.groupby('userID').apply(lambda x: x.index.values[0])\n",
    "    \n",
    "    lag_time_list = [0]*len(df)\n",
    "    for start_row, end_row in start_end_id_by_user_test [0][1:]:\n",
    "        start_time = df.time_stamp[start_row]\n",
    "        prev_time = df.time_stamp[start_row-1]\n",
    "        lag_time = start_time - prev_time\n",
    "        lag_time_list[start_row:end_row+1] = [lag_time]*(end_row-start_row+1)\n",
    "    \n",
    "    # 사용자가 바뀌는 부분 첫 시험지 lag_time은 0으로 변경\n",
    "    for user_start_idx in start_row_id_by_user:\n",
    "        start, end = start_end_id_by_user_test .loc[user_start_idx][0]\n",
    "        lag_time_list[start:end+1] = [0]*(end-start+1)\n",
    "        \n",
    "    df['lag_time'] = lag_time_list\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "91a2d39d-144c-4186-b504-6070c6d9b819",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_engineering(df, total_df):\n",
    "    df['time_stamp'] = df['Timestamp'].apply(convert_time)\n",
    "    df['day'] = df.Timestamp.apply(lambda x:x.split()[0])\n",
    "    df = create_elapsed_time(df)  # elapsed_time\n",
    "    df = create_lag_time(df)  # lag_time\n",
    "    \n",
    "    # -- 과거 feature\n",
    "    # 이전까지 맞은 문제 수 (피처 계산에만 사용)\n",
    "    df['prior_acc_count'] = df.groupby('userID')['answerCode'].transform(lambda x: x.cumsum().shift(1)).fillna(0)\n",
    "    # 이전까지 푼 문제 수  (피처 계산에만 사용)\n",
    "    df['prior_quest_count'] = df.groupby('userID')['answerCode'].cumcount().fillna(0)\n",
    "    # 이전 문제까지의 정답률\n",
    "    df['prior_acc'] = (df['prior_acc_count']/df['prior_quest_count']).fillna(0)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8f9d3e63-3328-4190-b090-fea57646d163",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = feature_engineering(df, total_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2e99e4cd-d7f0-41a1-be8c-93bea4751370",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 로그 스케일링\n",
    "# 로그 스케일 적용할 column\n",
    "log1p_cols = ['elapsed_time',\n",
    "              'lag_time'\n",
    "             ]\n",
    "\n",
    "df[log1p_cols] = np.log1p(df[log1p_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "96dda412-cfaa-40c1-a31a-ec735d7ec56c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 범주형, 수치형 feature\n",
    "cate_cols = ['assessmentItemID', 'testId', 'KnowledgeTag']\n",
    "\n",
    "cont_cols = ['elapsed_time',\n",
    "             'lag_time',\n",
    "             'prior_acc'\n",
    "            ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "76c56922-4fa0-4213-8c97-fd8e4f3d72ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "06596a9c-cd03-4e5f-b2cf-60e6cc574bc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_args(mode='train'):\n",
    "    parser = argparse.ArgumentParser()\n",
    "\n",
    "    \n",
    "    parser.add_argument('--seed', default=42, type=int, help='seed')\n",
    "    \n",
    "    parser.add_argument('--device', default='cpu', type=str, help='cpu or gpu')\n",
    "\n",
    "    parser.add_argument('--data_dir', default='/opt/ml/input/data/train_dataset', type=str, help='data directory')\n",
    "    parser.add_argument('--asset_dir', default='asset/', type=str, help='data directory')\n",
    "    \n",
    "    parser.add_argument('--file_name', default='train_data.csv', type=str, help='train file name')\n",
    "    \n",
    "    parser.add_argument('--model_dir', default='models/', type=str, help='model directory')\n",
    "    parser.add_argument('--model_name', default='model.pt', type=str, help='model file name')\n",
    "\n",
    "    parser.add_argument('--output_dir', default='output/', type=str, help='output directory')\n",
    "    parser.add_argument('--test_file_name', default='test_data.csv', type=str, help='test file name')\n",
    "    \n",
    "    parser.add_argument('--max_seq_len', default=20, type=int, help='max sequence length')\n",
    "    parser.add_argument('--num_workers', default=4, type=int, help='number of workers')\n",
    "\n",
    "    # 모델\n",
    "    parser.add_argument('--hidden_dim', default=64, type=int, help='hidden dimension size')\n",
    "    parser.add_argument('--n_layers', default=2, type=int, help='number of layers')\n",
    "    parser.add_argument('--n_heads', default=2, type=int, help='number of heads')\n",
    "    parser.add_argument('--drop_out', default=0.2, type=float, help='drop out rate')\n",
    "    \n",
    "    # 훈련\n",
    "    parser.add_argument('--n_epochs', default=100, type=int, help='number of epochs')\n",
    "    parser.add_argument('--batch_size', default=64, type=int, help='batch size')\n",
    "    parser.add_argument('--lr', default=0.0001, type=float, help='learning rate')\n",
    "    parser.add_argument('--clip_grad', default=10, type=int, help='clip grad')\n",
    "    parser.add_argument('--patience', default=5, type=int, help='for early stopping')\n",
    "    \n",
    "\n",
    "    parser.add_argument('--log_steps', default=50, type=int, help='print log per n steps')\n",
    "    \n",
    "\n",
    "    ### 중요 ###\n",
    "    parser.add_argument('--model', default='lstm', type=str, help='model type')\n",
    "    parser.add_argument('--optimizer', default='adam', type=str, help='optimizer type')\n",
    "    parser.add_argument('--scheduler', default='plateau', type=str, help='scheduler type')\n",
    "    \n",
    "    args = parser.parse_args([])\n",
    "\n",
    "    return args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "aece88fc-5fe1-4ba7-b63d-5a5b01c6209e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "args = parse_args(mode='train')\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "args.device = device\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f8d7f658-aacd-424f-9246-80db23d11393",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dkt.dataloader import Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "870f8e06-8c41-450c-8efe-2266ed61cccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess = Preprocess(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "818076d7-0e87-4aa0-9658-3fc51b6015e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 객체명._클래스명__private함수명으로 접근 가능\n",
    "df = preprocess._Preprocess__preprocessing(df, is_train=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7951c10a-2bcb-466c-b8e1-1c85d317bd4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 추후 feature를 embedding할 시에 embedding_layer의 input 크기를 결정할때 사용\n",
    "args.n_questions = len(np.load(os.path.join(args.asset_dir,'assessmentItemID_classes.npy')))\n",
    "args.n_test = len(np.load(os.path.join(args.asset_dir,'testId_classes.npy')))\n",
    "args.n_tag = len(np.load(os.path.join(args.asset_dir,'KnowledgeTag_classes.npy')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0fb8d18c-9c7a-4247-9fdc-e62eacabe37b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.sort_values(by=['userID','Timestamp'], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5440ab5d-31f0-4233-a14d-92a10d68d532",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ['assessmentItemID', 'testId', 'KnowledgeTag'], ['elapsed_time','lag_time','prior_acc']\n",
    "columns = ['userID','answerCode'] + cate_cols + cont_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8f6e68f9-a1db-4424-8b9b-db1929adfe99",
   "metadata": {},
   "outputs": [],
   "source": [
    "group = df[columns].groupby('userID').apply(\n",
    "                lambda r: tuple([r[col].values for col in columns[1:]])\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "10801cfb-c36f-43cb-80cb-498facb3fd16",
   "metadata": {},
   "outputs": [],
   "source": [
    "tot_data = group.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d1ac559c-a1df-489b-ad6f-615f2d7bc888",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, valid_data = preprocess.split_data(tot_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "764d1e3d-1131-43fe-946e-75e73c9eee7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loaders(args, train, valid):\n",
    "\n",
    "    pin_memory = True  # False\n",
    "    train_loader, valid_loader = None, None\n",
    "    \n",
    "    if train is not None:\n",
    "        trainset = CustomDKTDataset(train, args)\n",
    "        train_loader = torch.utils.data.DataLoader(trainset, num_workers=args.num_workers, shuffle=True,\n",
    "                            batch_size=args.batch_size, pin_memory=pin_memory, collate_fn=collate)\n",
    "    if valid is not None:\n",
    "        valset = CustomDKTDataset(valid, args)\n",
    "        valid_loader = torch.utils.data.DataLoader(valset, num_workers=args.num_workers, shuffle=False,\n",
    "                            batch_size=args.batch_size, pin_memory=pin_memory, collate_fn=collate)\n",
    "\n",
    "    return train_loader, valid_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2753dd91-1aab-455a-86a3-c2e0438749f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDKTDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data, args):\n",
    "        self.data = data\n",
    "        self.args = args\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        row = self.data[index]\n",
    "\n",
    "        # 각 data의 sequence length\n",
    "        seq_len = len(row[0])\n",
    "        \n",
    "        # 'assessmentItemID', 'testId', 'KnowledgeTag'\n",
    "        correct, question, test, tag = row[0], row[1], row[2], row[3]  \n",
    "        cate_cols = [test, question, tag, correct]\n",
    "\n",
    "        # 'elapsed_time','lag_time','prior_acc'\n",
    "        cont_cols = list(row[4:])\n",
    "\n",
    "        # max seq len을 고려하여서 이보다 길면 자르고 아닐 경우 그대로 냅둔다\n",
    "        if seq_len > args.max_seq_len:\n",
    "            for i, col in enumerate(cate_cols):\n",
    "                cate_cols[i] = col[-args.max_seq_len:]\n",
    "\n",
    "            for i, col in enumerate(cont_cols):\n",
    "                cont_cols[i] = col[-args.max_seq_len:]\n",
    "            mask = np.ones(args.max_seq_len, dtype=np.int16)\n",
    "        else:\n",
    "            # 원래 sequence가 있는 길이부분만 1\n",
    "            mask = np.zeros(args.max_seq_len, dtype=np.int16)\n",
    "            mask[-seq_len:] = 1\n",
    "\n",
    "        # mask도 columns 목록에 포함시킴\n",
    "        cate_cols.append(mask)\n",
    "\n",
    "        # np.array -> torch.tensor 형변환\n",
    "        for i, col in enumerate(cate_cols):\n",
    "            cate_cols[i] = torch.tensor(col)\n",
    "\n",
    "        for i, col in enumerate(cont_cols):\n",
    "            cont_cols[i] = torch.tensor(col)\n",
    "\n",
    "        return cont_cols+cate_cols\n",
    "    \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "21e576e3-8ea4-46fa-b1e8-e0ccdc98c2f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate(batch):\n",
    "    col_n = len(batch[0])  \n",
    "    col_list = [[] for _ in range(col_n)]\n",
    "    # input으로 들어오는 batch는 DKTDataset의 output으로  \n",
    "    # 기존 baseline에서 맨 마지막에 위치하는 tensor는 mask\n",
    "    # mask는 DKTDataset에서 생성될 때 args.max_seq_len에 맞춰서 생성됨\n",
    "    # DKTDataset의 output을 cont+cate로 수정하면 그대로 mask가 맨 마지막에 위치함\n",
    "    max_seq_len = len(batch[0][-1]) \n",
    "\n",
    "    # batch의 값들을 각 column끼리 그룹화\n",
    "    for row in batch:  # row: feature values\n",
    "        for i, col in enumerate(row):  # col: feature value\n",
    "            pre_padded = torch.zeros(max_seq_len)\n",
    "            pre_padded[-len(col):] = col  # 앞부분이 0으로 padding됨\n",
    "            col_list[i].append(pre_padded)\n",
    "\n",
    "\n",
    "    for i, _ in enumerate(col_list):\n",
    "        col_list[i] =torch.stack(col_list[i])\n",
    "    \n",
    "    return tuple(col_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1d8a6f05-a478-4d38-9b1b-facdbd7224eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 배치 전처리\n",
    "def process_batch(batch, args):\n",
    "\n",
    "    test, question, tag, correct, mask = batch[-5:]\n",
    "    cont = batch[:-5] \n",
    "    \n",
    "    # change to float\n",
    "    mask = mask.type(torch.FloatTensor)\n",
    "    correct = correct.type(torch.FloatTensor)\n",
    "\n",
    "    # interaction: 과거 정답 여부를 다음 시퀀스에 추가적인 feature로 사용하게끔 한칸 시프트 해준 feature\n",
    "    #  interaction을 임시적으로 correct를 한칸 우측으로 이동한 것으로 사용\n",
    "    #    saint의 경우 decoder에 들어가는 input이다\n",
    "    interaction = correct + 1 # 패딩을 위해 correct값에 1을 더해준다. (정답 2, 오답 1)\n",
    "    interaction = interaction.roll(shifts=1, dims=1)\n",
    "    interaction_mask = mask.roll(shifts=1, dims=1)\n",
    "    interaction_mask[:, 0] = 0\n",
    "    interaction = (interaction * interaction_mask).to(torch.int64)  # 가장 마지막으로 푼 문제를 제외하고 정답 2, 오답 1\n",
    "    # print(interaction)\n",
    "    # exit()\n",
    "    #  test_id, question_id, tag\n",
    "    test = ((test + 1) * mask).to(torch.int64)\n",
    "    question = ((question + 1) * mask).to(torch.int64)\n",
    "    tag = ((tag + 1) * mask).to(torch.int64)\n",
    "    \n",
    "    # cont features도 padding을 위해 1을 더함\n",
    "    for i in range(len(cont)):\n",
    "        cont[i] = ((cont[i]+1)*mask).to(torch.float32)\n",
    "\n",
    "\n",
    "    # device memory로 이동\n",
    "    test = test.to(args.device)\n",
    "    question = question.to(args.device)\n",
    "    tag = tag.to(args.device)\n",
    "    correct = correct.to(args.device)\n",
    "    mask = mask.to(args.device)\n",
    "    interaction = interaction.to(args.device)\n",
    "    for i in range(len(cont)):\n",
    "        cont[i] = cont[i].to(args.device)\n",
    "\n",
    "    return (test, question, tag, correct, mask, interaction, cont)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3ccdde9f-9ea2-48cc-9a76-0b976de2ac32",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F \n",
    "import numpy as np\n",
    "import copy\n",
    "import math\n",
    "\n",
    "try:\n",
    "    from transformers.modeling_bert import BertConfig, BertEncoder, BertModel    \n",
    "except:\n",
    "    from transformers.models.bert.modeling_bert import BertConfig, BertEncoder, BertModel    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "5b89c0da-fb6b-40c2-b239-cebfc2d52f56",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "\n",
    "    def __init__(self, args, cont_col_count=3):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.args = args\n",
    "        self.device = args.device\n",
    "    \n",
    "        self.cont_col_count = cont_col_count\n",
    "        \n",
    "        self.hidden_dim = self.args.hidden_dim\n",
    "        self.n_layers = self.args.n_layers\n",
    "        \n",
    "\n",
    "        # Embedding \n",
    "        # interaction은 현재 correct로 구성되어있다. correct(1, 2) + padding(0)\n",
    "        self.embedding_interaction = nn.Embedding(3, self.hidden_dim//3)\n",
    "        self.embedding_test = nn.Embedding(self.args.n_test + 1, self.hidden_dim//3)\n",
    "        self.embedding_question = nn.Embedding(self.args.n_questions + 1, self.hidden_dim//3)\n",
    "        self.embedding_tag = nn.Embedding(self.args.n_tag + 1, self.hidden_dim//3)\n",
    "        \n",
    "        # embedding cate projection\n",
    "        #self.cate_proj = nn.Linear((self.hidden_dim//3)*4, self.hidden_dim)\n",
    "        self.cate_proj = nn.Sequential(\n",
    "                            nn.Linear((self.hidden_dim//3)*4, self.hidden_dim),\n",
    "                            nn.LayerNorm(self.hidden_dim)\n",
    "                            )  \n",
    "        \n",
    "        # cont embedding\n",
    "        self.cont_bn = nn.BatchNorm1d(self.cont_col_count)\n",
    "        self.cont_proj =  nn.Sequential(\n",
    "                nn.Linear(self.cont_col_count, self.hidden_dim),\n",
    "                nn.LayerNorm(self.hidden_dim)\n",
    "                )\n",
    "        \n",
    "        # embedding combination projection\n",
    "        self.comb_proj = nn.Linear(self.hidden_dim*2, self.hidden_dim)\n",
    "\n",
    "        self.lstm = nn.LSTM(self.hidden_dim,\n",
    "                            self.hidden_dim,\n",
    "                            self.n_layers,\n",
    "                            batch_first=True)\n",
    "        \n",
    "        # Fully connected layer\n",
    "        self.fc = nn.Linear(self.hidden_dim, 1)\n",
    "\n",
    "        self.activation = nn.Sigmoid()\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        h = torch.zeros(\n",
    "            self.n_layers,\n",
    "            batch_size,\n",
    "            self.hidden_dim)\n",
    "        h = h.to(self.device)\n",
    "\n",
    "        c = torch.zeros(\n",
    "            self.n_layers,\n",
    "            batch_size,\n",
    "            self.hidden_dim)\n",
    "        c = c.to(self.device)\n",
    "\n",
    "        return (h, c)\n",
    "\n",
    "    def forward(self, input):\n",
    "\n",
    "        test, question, tag, _, mask, interaction, cont = input\n",
    "\n",
    "        batch_size = interaction.size(0)\n",
    "\n",
    "        # Embedding\n",
    "\n",
    "        embed_interaction = self.embedding_interaction(interaction)\n",
    "        embed_test = self.embedding_test(test)\n",
    "        embed_question = self.embedding_question(question)\n",
    "        embed_tag = self.embedding_tag(tag)\n",
    "        \n",
    "        cate_embed = torch.cat([embed_interaction,\n",
    "                                embed_test,\n",
    "                                embed_question,\n",
    "                                embed_tag,], 2\n",
    "                                )\n",
    "        cate_embed = self.cate_proj(cate_embed)\n",
    "        \n",
    "\n",
    "        cont_x = torch.cat(cont, 1)\n",
    "        cont_bn_x = self.cont_bn(cont_x.view(-1, self.cont_col_count))\n",
    "        # view 할 때 맨 앞을 -1로 해야함,\n",
    "        # 맨 마지막 batch는 실제 batch보다 작아서 고정된 값으로 놓으면 에러남\n",
    "        cont_bn_x = cont_bn_x.view(-1, args.max_seq_len, self.cont_col_count)\n",
    "        cont_embed = self.cont_proj(cont_bn_x)\n",
    "        \n",
    "        comb_embed = torch.cat([cate_embed, cont_embed], 2)\n",
    "        X = self.comb_proj(comb_embed)\n",
    "\n",
    "        hidden = self.init_hidden(batch_size)\n",
    "        out, hidden = self.lstm(X, hidden)\n",
    "        out = out.contiguous().view(batch_size, -1, self.hidden_dim)  # .contiguous(): 새로운 텐서를 반환\n",
    "\n",
    "        out = self.fc(out)\n",
    "        preds = self.activation(out).view(batch_size, -1)\n",
    "\n",
    "        return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "05695ddf-0b05-4a21-bd47-349823a2349f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train(train_loader, model, optimizer, args):\n",
    "    model.train()\n",
    "\n",
    "    total_preds = []\n",
    "    total_targets = []\n",
    "    losses = []\n",
    "    for step, batch in enumerate(train_loader):\n",
    "        input = process_batch(batch, args)\n",
    "        preds = model(input)\n",
    "        targets = input[3] # correct\n",
    "\n",
    "\n",
    "        loss = compute_loss(preds, targets)\n",
    "        update_params(loss, model, optimizer, args)\n",
    "\n",
    "\n",
    "        if step % args.log_steps == 0:\n",
    "            print(f\"Training steps: {step} Loss: {str(loss.item())}\")\n",
    "        \n",
    "        # predictions\n",
    "        preds = preds[:,-1]\n",
    "        targets = targets[:,-1]\n",
    "\n",
    "        if args.device == 'cuda':\n",
    "            preds = preds.to('cpu').detach().numpy()\n",
    "            targets = targets.to('cpu').detach().numpy()\n",
    "        else: # cpu\n",
    "            preds = preds.detach().numpy()\n",
    "            targets = targets.detach().numpy()\n",
    "        \n",
    "        total_preds.append(preds)\n",
    "        total_targets.append(targets)\n",
    "        losses.append(loss)\n",
    "      \n",
    "\n",
    "    total_preds = np.concatenate(total_preds)\n",
    "    total_targets = np.concatenate(total_targets)\n",
    "\n",
    "    # Train AUC / ACC\n",
    "    auc, acc = get_metric(total_targets, total_preds)\n",
    "    loss_avg = sum(losses)/len(losses)\n",
    "    print(f'TRAIN AUC : {auc} ACC : {acc}')\n",
    "    return auc, acc, loss_avg\n",
    "    \n",
    "\n",
    "def validate(valid_loader, model, args):\n",
    "    model.eval()\n",
    "\n",
    "    total_preds = []\n",
    "    total_targets = []\n",
    "    for step, batch in enumerate(valid_loader):\n",
    "        input = process_batch(batch, args)\n",
    "\n",
    "        preds = model(input)\n",
    "        targets = input[3] # correct\n",
    "\n",
    "\n",
    "        # predictions\n",
    "        preds = preds[:,-1]\n",
    "        targets = targets[:,-1]\n",
    "    \n",
    "        if args.device == 'cuda':\n",
    "            preds = preds.to('cpu').detach().numpy()\n",
    "            targets = targets.to('cpu').detach().numpy()\n",
    "        else: # cpu\n",
    "            preds = preds.detach().numpy()\n",
    "            targets = targets.detach().numpy()\n",
    "\n",
    "        total_preds.append(preds)\n",
    "        total_targets.append(targets)\n",
    "\n",
    "    total_preds = np.concatenate(total_preds)\n",
    "    total_targets = np.concatenate(total_targets)\n",
    "\n",
    "    # Train AUC / ACC\n",
    "    auc, acc = get_metric(total_targets, total_preds)\n",
    "    \n",
    "    print(f'VALID AUC : {auc} ACC : {acc}\\n')\n",
    "\n",
    "    return auc, acc, total_preds, total_targets\n",
    "\n",
    "\n",
    "\n",
    "def inference(args, test_data):\n",
    "    \n",
    "    model = load_model(args)\n",
    "    model.eval()\n",
    "    _, test_loader = get_loaders(args, None, test_data)\n",
    "    \n",
    "    \n",
    "    total_preds = []\n",
    "    \n",
    "    for step, batch in enumerate(test_loader):\n",
    "        input = process_batch(batch, args)\n",
    "\n",
    "        preds = model(input)\n",
    "        \n",
    "\n",
    "        # predictions\n",
    "        preds = preds[:,-1]\n",
    "        \n",
    "\n",
    "        if args.device == 'cuda':\n",
    "            preds = preds.to('cpu').detach().numpy()\n",
    "        else: # cpu\n",
    "            preds = preds.detach().numpy()\n",
    "            \n",
    "        total_preds+=list(preds)\n",
    "\n",
    "    write_path = os.path.join(args.output_dir, \"output.csv\")\n",
    "    if not os.path.exists(args.output_dir):\n",
    "        os.makedirs(args.output_dir)    \n",
    "    with open(write_path, 'w', encoding='utf8') as w:\n",
    "        print(\"writing prediction : {}\".format(write_path))\n",
    "        w.write(\"id,prediction\\n\")\n",
    "        for id, p in enumerate(total_preds):\n",
    "            w.write('{},{}\\n'.format(id,p))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "ab6037a1-b17e-4fa4-9599-214759c96dc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "#from dkt.dataloader import get_loaders\n",
    "from dkt.optimizer import get_optimizer\n",
    "from dkt.scheduler import get_scheduler\n",
    "from dkt.criterion import get_criterion\n",
    "from dkt.metric import get_metric\n",
    "from dkt.trainer import get_lr, compute_loss, update_params, save_checkpoint, load_model\n",
    "\n",
    "import wandb\n",
    "import time\n",
    "import datetime\n",
    "import gc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "e78c924f-1c1a-4bb8-90b7-d42a7774a2a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, valid_loader = get_loaders(args, train_data, valid_data)\n",
    "\n",
    "# only when using warmup scheduler\n",
    "args.total_steps = int(len(train_loader.dataset) / args.batch_size) * (args.n_epochs)\n",
    "args.warmup_steps = args.total_steps // 10\n",
    "\n",
    "model = LSTM(args)\n",
    "model.to(args.device)\n",
    "\n",
    "optimizer = get_optimizer(model, args)\n",
    "scheduler = get_scheduler(optimizer, args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "38c67b9e-068a-4f79-adbb-551c94c5eec0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "74\n",
      "32\n"
     ]
    }
   ],
   "source": [
    "print(len(train_loader))\n",
    "print(len(valid_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "31188bdd-6ed6-4948-a56d-27778cd01596",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.10.31 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.10.30<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">cont_cate_lstm_100epochs</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/dhh0/dkt\" target=\"_blank\">https://wandb.ai/dhh0/dkt</a><br/>\n",
       "                Run page: <a href=\"https://wandb.ai/dhh0/dkt/runs/34rfx3ii\" target=\"_blank\">https://wandb.ai/dhh0/dkt/runs/34rfx3ii</a><br/>\n",
       "                Run data is saved locally in <code>/opt/ml/my_code/wandb/run-20210607_180619-34rfx3ii</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h1>Run(34rfx3ii)</h1><iframe src=\"https://wandb.ai/dhh0/dkt/runs/34rfx3ii\" style=\"border:none;width:100%;height:400px\"></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7f81e74eee10>"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.init(project='dkt', config=vars(args), tags=[args.model], name=f'cont_cate_{args.model}_{args.n_epochs}epochs')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "9dd77f81-0e7c-49e6-970f-16fb46edfa69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Training: Epoch 1\n",
      "Training steps: 0 Loss: 0.7000912427902222\n",
      "Training steps: 50 Loss: 0.6918453574180603\n",
      "TRAIN AUC : 0.5685841187994971 ACC : 0.5420221843003413\n",
      "VALID AUC : 0.7147598687244672 ACC : 0.6462686567164179\n",
      "\n",
      "<<<<<<<<<<  1 EPOCH spent : 0:00:02  >>>>>>>>>>\n",
      "saving model ...\n",
      "Start Training: Epoch 2\n",
      "Training steps: 0 Loss: 0.6881948113441467\n",
      "Training steps: 50 Loss: 0.6821510791778564\n",
      "TRAIN AUC : 0.7183864265612308 ACC : 0.6452645051194539\n",
      "VALID AUC : 0.7352134020853728 ACC : 0.6810945273631841\n",
      "\n",
      "<<<<<<<<<<  2 EPOCH spent : 0:00:02  >>>>>>>>>>\n",
      "saving model ...\n",
      "Start Training: Epoch 3\n",
      "Training steps: 0 Loss: 0.6826542615890503\n",
      "Training steps: 50 Loss: 0.6533534526824951\n",
      "TRAIN AUC : 0.7325656596229215 ACC : 0.681740614334471\n",
      "VALID AUC : 0.7364095209474532 ACC : 0.6761194029850747\n",
      "\n",
      "<<<<<<<<<<  3 EPOCH spent : 0:00:02  >>>>>>>>>>\n",
      "saving model ...\n",
      "Start Training: Epoch 4\n",
      "Training steps: 0 Loss: 0.6134753823280334\n",
      "Training steps: 50 Loss: 0.6390873193740845\n",
      "TRAIN AUC : 0.7387913469419012 ACC : 0.6860068259385665\n",
      "VALID AUC : 0.7362538072979119 ACC : 0.681592039800995\n",
      "\n",
      "<<<<<<<<<<  4 EPOCH spent : 0:00:02  >>>>>>>>>>\n",
      "Start Training: Epoch 5\n",
      "Training steps: 0 Loss: 0.6289911270141602\n",
      "Training steps: 50 Loss: 0.6434364318847656\n",
      "TRAIN AUC : 0.7438004315771528 ACC : 0.6860068259385665\n",
      "VALID AUC : 0.7375649757354495 ACC : 0.6800995024875622\n",
      "\n",
      "<<<<<<<<<<  5 EPOCH spent : 0:00:02  >>>>>>>>>>\n",
      "saving model ...\n",
      "Start Training: Epoch 6\n",
      "Training steps: 0 Loss: 0.6421188116073608\n",
      "Training steps: 50 Loss: 0.6907533407211304\n",
      "TRAIN AUC : 0.7464333804050846 ACC : 0.6911262798634812\n",
      "VALID AUC : 0.7386510040554976 ACC : 0.682089552238806\n",
      "\n",
      "<<<<<<<<<<  6 EPOCH spent : 0:00:02  >>>>>>>>>>\n",
      "saving model ...\n",
      "Start Training: Epoch 7\n",
      "Training steps: 0 Loss: 0.5424964427947998\n",
      "Training steps: 50 Loss: 0.6474435925483704\n",
      "TRAIN AUC : 0.7485950039520214 ACC : 0.6917662116040956\n",
      "VALID AUC : 0.7391449237846153 ACC : 0.6830845771144278\n",
      "\n",
      "<<<<<<<<<<  7 EPOCH spent : 0:00:02  >>>>>>>>>>\n",
      "saving model ...\n",
      "Start Training: Epoch 8\n",
      "Training steps: 0 Loss: 0.5442992448806763\n",
      "Training steps: 50 Loss: 0.6234670877456665\n",
      "TRAIN AUC : 0.7499982222740852 ACC : 0.6968856655290102\n",
      "VALID AUC : 0.7394563510836977 ACC : 0.6835820895522388\n",
      "\n",
      "<<<<<<<<<<  8 EPOCH spent : 0:00:02  >>>>>>>>>>\n",
      "saving model ...\n",
      "Start Training: Epoch 9\n",
      "Training steps: 0 Loss: 0.7076926827430725\n",
      "Training steps: 50 Loss: 0.6139754056930542\n",
      "TRAIN AUC : 0.7523927734985737 ACC : 0.6938993174061433\n",
      "VALID AUC : 0.7397211634685861 ACC : 0.6825870646766169\n",
      "\n",
      "<<<<<<<<<<  9 EPOCH spent : 0:00:02  >>>>>>>>>>\n",
      "saving model ...\n",
      "Start Training: Epoch 10\n",
      "Training steps: 0 Loss: 0.6185754537582397\n",
      "Training steps: 50 Loss: 0.635185956954956\n",
      "TRAIN AUC : 0.7532535575030518 ACC : 0.6943259385665529\n",
      "VALID AUC : 0.7396071056998139 ACC : 0.6830845771144278\n",
      "\n",
      "<<<<<<<<<<  10 EPOCH spent : 0:00:02  >>>>>>>>>>\n",
      "Start Training: Epoch 11\n",
      "Training steps: 0 Loss: 0.6991963982582092\n",
      "Training steps: 50 Loss: 0.546909511089325\n",
      "TRAIN AUC : 0.7539311901555921 ACC : 0.6998720136518771\n",
      "VALID AUC : 0.7400256481208735 ACC : 0.6835820895522388\n",
      "\n",
      "<<<<<<<<<<  11 EPOCH spent : 0:00:02  >>>>>>>>>>\n",
      "saving model ...\n",
      "Start Training: Epoch 12\n",
      "Training steps: 0 Loss: 0.5411653518676758\n",
      "Training steps: 50 Loss: 0.5641289949417114\n",
      "TRAIN AUC : 0.7556574987669875 ACC : 0.6994453924914675\n",
      "VALID AUC : 0.7399458076827329 ACC : 0.681592039800995\n",
      "\n",
      "<<<<<<<<<<  12 EPOCH spent : 0:00:02  >>>>>>>>>>\n",
      "Start Training: Epoch 13\n",
      "Training steps: 0 Loss: 0.5937364101409912\n",
      "Training steps: 50 Loss: 0.5852802991867065\n",
      "TRAIN AUC : 0.7561327441615379 ACC : 0.7007252559726962\n",
      "VALID AUC : 0.7393938672625441 ACC : 0.681592039800995\n",
      "\n",
      "<<<<<<<<<<  13 EPOCH spent : 0:00:02  >>>>>>>>>>\n",
      "Start Training: Epoch 14\n",
      "Training steps: 0 Loss: 0.5659475326538086\n",
      "Training steps: 50 Loss: 0.607698917388916\n",
      "TRAIN AUC : 0.7561997507537102 ACC : 0.7017918088737202\n",
      "VALID AUC : 0.7398471229175778 ACC : 0.6830845771144278\n",
      "\n",
      "<<<<<<<<<<  14 EPOCH spent : 0:00:02  >>>>>>>>>>\n",
      "Start Training: Epoch 15\n",
      "Training steps: 0 Loss: 0.623885989189148\n",
      "Training steps: 50 Loss: 0.5673843622207642\n",
      "TRAIN AUC : 0.7569713749661547 ACC : 0.7002986348122867\n",
      "VALID AUC : 0.739882827958237 ACC : 0.681592039800995\n",
      "\n",
      "<<<<<<<<<<  15 EPOCH spent : 0:00:02  >>>>>>>>>>\n",
      "Start Training: Epoch 16\n",
      "Training steps: 0 Loss: 0.5228726863861084\n",
      "Training steps: 50 Loss: 0.5708659887313843\n",
      "TRAIN AUC : 0.7578440104767312 ACC : 0.7015784982935154\n",
      "VALID AUC : 0.7399532462328702 ACC : 0.6810945273631841\n",
      "\n",
      "<<<<<<<<<<  16 EPOCH spent : 0:00:02  >>>>>>>>>>\n",
      "EarlyStopping counter: 5 out of 5\n"
     ]
    }
   ],
   "source": [
    "best_auc = -1\n",
    "early_stopping_counter = 0\n",
    "for epoch in range(args.n_epochs):\n",
    "\n",
    "    print(f\"Start Training: Epoch {epoch + 1}\")\n",
    "    start = time.time()\n",
    "    ### TRAIN\n",
    "    train_auc, train_acc, train_loss = train(train_loader, model, optimizer, args)\n",
    "\n",
    "    ### VALID\n",
    "    auc, acc,_ , _ = validate(valid_loader, model, args)\n",
    "\n",
    "    sec = time.time() - start\n",
    "    times = str(datetime.timedelta(seconds=sec)).split(\".\")\n",
    "    times = times[0]\n",
    "    print(f'<<<<<<<<<<  {epoch + 1} EPOCH spent : {times}  >>>>>>>>>>')\n",
    "\n",
    "    ### TODO: model save or early stopping\n",
    "    wandb.log({\"epoch\": epoch, \"train_loss\": train_loss, \"train_auc\": train_auc, \"train_acc\":train_acc,\n",
    "              \"valid_auc\":auc, \"valid_acc\":acc, \"Learning_rate\": get_lr(optimizer),})\n",
    "    if auc > best_auc:\n",
    "        best_auc = auc\n",
    "        # torch.nn.DataParallel로 감싸진 경우 원래의 model을 가져옵니다.\n",
    "        model_to_save = model.module if hasattr(model, 'module') else model\n",
    "        save_checkpoint({\n",
    "            'epoch': epoch + 1,\n",
    "            'state_dict': model_to_save.state_dict(),\n",
    "            },\n",
    "            args.model_dir, 'model.pt',\n",
    "        )\n",
    "        early_stopping_counter = 0\n",
    "    else:\n",
    "        early_stopping_counter += 1\n",
    "        if early_stopping_counter >= args.patience:\n",
    "            print(f'EarlyStopping counter: {early_stopping_counter} out of {args.patience}')\n",
    "            break\n",
    "\n",
    "    # scheduler\n",
    "    if args.scheduler == 'plateau':\n",
    "        scheduler.step(best_auc)\n",
    "    else:\n",
    "        scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ff0f403-dbee-4cc5-8770-92321e93cc73",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
